# api/main.py ‚Äî fast & tolerant (rule-based parser + FTS + pgvector hybrid rank)
import os, time, json, re, unicodedata
from typing import List, Optional, Dict  # ‚Üê TH√äM Dict
from functools import lru_cache

import requests
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from psycopg_pool import ConnectionPool
from sentence_transformers import SentenceTransformer

# ====== CONFIG ======
DB_URL = (
    "postgresql://neondb_owner:npg_LoMRfn9gqI0A@ep-noisy-darkness-a1wr0h4z-pooler.ap-southeast-1.aws.neon.tech/neondb"
    "?sslmode=require"
    "&connect_timeout=15"
    "&hostaddr=13.228.184.177"
    "&keepalives=1&keepalives_idle=30&keepalives_interval=10&keepalives_count=5"
)
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434/api/generate")

# Ng∆∞·ª°ng semantic (0..1). ƒê·∫∑t 0.0 n·∫øu kh√¥ng mu·ªën ch·∫∑n.
SEM_MIN_SIM = float(os.getenv("SEM_MIN_SIM", "0.15"))

# D√πng 1 model nh·ªè cho t·ªëc ƒë·ªô
PARSE_MODEL = CHAT_MODEL = os.getenv("LLM_MODEL", "qwen2.5:3b-instruct-q4_0")

GEN_PARSE_OPTS = {"temperature": 0.0, "num_predict": 120, "keep_alive": "30m", "num_thread": os.cpu_count() or 4}
GEN_CHAT_OPTS = {"temperature": 0.25, "num_predict": 96, "keep_alive": "30m", "num_thread": os.cpu_count() or 4}

# Embedding model (384 chi·ªÅu)
_embed_model = SentenceTransformer("intfloat/multilingual-e5-small")

# DB pool
pool = ConnectionPool(conninfo=DB_URL, min_size=1, max_size=10)

app = FastAPI(title="Boarding Chatbot API")


# ====== MODELS ======
class Search(BaseModel):
    province: Optional[str] = None
    district: Optional[str] = None
    ward: Optional[str] = None
    price_min: Optional[float] = None
    price_max: Optional[float] = None
    area_min: Optional[float] = None
    area_max: Optional[float] = None
    amenities: Optional[List[str]] = None
    targets: Optional[List[str]] = None
    env_types: Optional[List[str]] = None
    keywords: Optional[List[str]] = None
    limit: int = 10
    page: int = 1
    semantic_query: Optional[str] = None


class ChatHistory(BaseModel):
    message: str
    history: List[dict] = []


# ====== STARTUP ======
PROVINCES: List[str] = []
DISTRICTS: List[str] = []
WARDS: List[str] = []


@app.on_event("startup")
def _setup():
    try:
        with pool.connection() as conn:
            with conn.cursor() as cur:
                cur.execute("CREATE EXTENSION IF NOT EXISTS unaccent;")
                cur.execute("SELECT DISTINCT province FROM boarding_zones WHERE province IS NOT NULL LIMIT 500")
                globals()["PROVINCES"] = [r[0] for r in cur.fetchall()]
                cur.execute("SELECT DISTINCT district FROM boarding_zones WHERE district IS NOT NULL LIMIT 5000")
                globals()["DISTRICTS"] = [r[0] for r in cur.fetchall()]
                cur.execute("SELECT DISTINCT ward FROM boarding_zones WHERE ward IS NOT NULL LIMIT 10000")
                globals()["WARDS"] = [r[0] for r in cur.fetchall()]
        print(f"[VOCAB] provinces={len(PROVINCES)} districts={len(DISTRICTS)} wards={len(WARDS)}")
    except Exception as e:
        print("[WARN] startup:", e)


# ====== UTILS ======
def timing(label: str, t0: float) -> float:
    t1 = time.perf_counter()
    print(f"[TIMING] {label}: {t1 - t0:.2f}s")
    return t1


def ollama_generate(model: str, system: str, prompt: str, *, as_json=False, timeout=60) -> str:
    payload = {
        "model": model,
        "system": system or "",
        "prompt": prompt,
        "options": GEN_PARSE_OPTS if as_json else GEN_CHAT_OPTS,
        "stream": False,
    }
    if as_json: payload["format"] = "json"
    t0 = time.perf_counter()
    r = requests.post(OLLAMA_URL, json=payload, timeout=timeout)
    print(f"[OLLAMA] {model} {r.status_code} in {time.perf_counter() - t0:.2f}s")
    r.raise_for_status()
    return r.json().get("response", "")


@lru_cache(maxsize=512)
def embed_vec_literal(text: str) -> str:
    vec = _embed_model.encode([text], normalize_embeddings=True)[0].tolist()
    return "[" + ",".join(f"{x:.6f}" for x in vec) + "]"


def widen_price(pmin: Optional[float], pmax: Optional[float]):
    if pmin is not None and pmax is not None and pmin == pmax:
        delta = max(500_000, int(pmin * 0.15))  # ¬±500k ho·∫∑c 15%
        return pmin - delta, pmax + delta
    return pmin, pmax


# ====== PARSER RULES ======
def _strip_accents(s: str) -> str:
    return "".join(c for c in unicodedata.normalize("NFD", s) if unicodedata.category(c) != "Mn")


def _contains(term: str, text: str) -> bool:
    return _strip_accents(term.lower()) in _strip_accents(text.lower())


def _parse_price(msg: str):
    txt = _strip_accents(msg.lower())
    m = re.search(r'(\d+(?:[.,]\d+)?)\s*(tr|trieu|million)', txt)
    if m:
        val = float(m.group(1).replace(",", "."))
        v = val * 1_000_000
        return max(0, v * 0.85), v * 1.15
    m = re.search(r'(\d[\d .]{5,})\s*(?:d|vnd)?', txt)
    if m:
        v = float(re.sub(r"[ .]", "", m.group(1)))
        return max(0, v * 0.85), v * 1.15
    return None, None


def _find_one(cands: List[str], msg: str) -> Optional[str]:
    for c in cands:
        if c and _contains(c, msg):
            return c
    return None


# --- GEO NORMALIZATION ---
GEO_PREFIX_RE = re.compile(
    r'\b('
    r'q\.?|quan|qu·∫≠n|h\.?|huyen|huy·ªán|tp|t\.p\.?|thanh pho|th√†nh ph·ªë|'
    r'thi xa|tx|phuong|p\.?|ph∆∞·ªùng'
    r')\b',
    flags=re.IGNORECASE
)


def _canon_geo(s: str) -> str:
    s = _strip_accents((s or "").lower())
    s = GEO_PREFIX_RE.sub(" ", s)
    s = re.sub(r'[^a-z0-9 ]+', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip()
    return s


def _find_one_geo(cands: List[str], msg: str) -> Optional[str]:
    ms = _canon_geo(msg)
    if not ms:
        return None
    for c in cands:
        cs = _canon_geo(c or "")
        if not cs:
            continue
        if cs == ms or cs in ms or ms in cs:
            return c
    return None


# ====== CORE ======
# Domain phrases ‚Üí ch·ªâ gi·ªØ keyword c√≥ nghƒ©a
PET_SYNS = [
    "th√∫ c∆∞ng", "nu√¥i th√∫ c∆∞ng", "cho nu√¥i th√∫ c∆∞ng",
    "pet", "pet friendly", "pet-friendly",
    "nu√¥i ch√≥", "nu√¥i m√®o", "cho nu√¥i ch√≥", "cho nu√¥i m√®o"
]
AMENITY_PHRASES = [
    "n·ªôi th·∫•t", "ƒë·∫ßy ƒë·ªß n·ªôi th·∫•t", "full n·ªôi th·∫•t", "full nt", "c∆° b·∫£n n·ªôi th·∫•t",
    "m√°y l·∫°nh", "ƒëi·ªÅu h√≤a", "m√°y gi·∫∑t", "t·ªß l·∫°nh", "b√†n h·ªçc", "gi∆∞·ªùng", "t·ªß qu·∫ßn √°o",
    "ban c√¥ng", "c·ª≠a s·ªï", "c·ª≠a s·ªï l·ªõn", "√°nh s√°ng t·ª± nhi√™n",
    "thang m√°y",
    "g√°c", "g√°c l·ª≠ng",
    "b·∫øp", "n·∫•u ƒÉn", "cho n·∫•u ƒÉn", "khu b·∫øp chung",
    "wc ri√™ng", "toilet ri√™ng", "nh√† v·ªá sinh ri√™ng",
    "ch·ªó ƒë·ªÉ xe", "b√£i xe", "gi·ªØ xe",
    "gi·ªù gi·∫•c t·ª± do", "t·ª± do gi·ªù gi·∫•c",
    "·ªü gh√©p", "share ph√≤ng", "share room",
    "g·∫ßn tr∆∞·ªùng", "g·∫ßn ƒë·∫°i h·ªçc", "g·∫ßn b·ªánh vi·ªán", "g·∫ßn ch·ª£", "g·∫ßn b·∫øn xe"
]
VN_STOPWORDS = {
    "c√≥", "n√†o", "·ªü", "o", "cho", "kh√¥ng", "khong", "t√¨m", "tim", "ph√≤ng", "phong",
    "tr·ªç", "tro", "c·∫ßn", "can", "gi√∫p", "giup", "gi√°", "gia", "kho·∫£ng", "khoang",
    "qu·∫≠n", "quan", "huy·ªán", "huyen", "ph∆∞·ªùng", "phuong", "tp", "t p", "thanh", "pho",
    "t√¥i", "toi", "m√¨nh", "minh", "mu·ªën", "muon", "c·∫ßn t√¨m", "can tim", "tim phong",
    "phong tro", "tro", "nha", "nh√†", "cho thue", "thu√™", "thue", "gia re", "r·∫ª",
    "tot", "t·ªët", "dep", "ƒë·∫πp", "san", "s·∫µn", "nao", "n√†o", "day", "ƒë·∫ßy", "du", "ƒë·ªß"
}


def _remove_geo_from_text(msg: str) -> str:
    ms = _canon_geo(msg)
    for x in (PROVINCES + DISTRICTS + WARDS):
        cx = _canon_geo(x or "")
        if not cx:
            continue
        ms = re.sub(rf'\b{re.escape(cx)}\b', ' ', ms)
    ms = re.sub(r'\s+', ' ', ms).strip()
    return ms


def _pick_phrases(msg: str, phrases: List[str]) -> List[str]:
    s = _strip_accents(msg.lower())
    out = []
    for p in phrases:
        if _strip_accents(p) in s:
            out.append(p)
    seen, uniq = set(), []
    for k in out:
        if k not in seen:
            seen.add(k)
            uniq.append(k)
    return uniq


def _extract_regular_keywords(text: str) -> List[str]:
    """Tr√≠ch xu·∫•t keyword th√¥ng th∆∞·ªùng t·ª´ text, lo·∫°i b·ªè stopwords"""
    if not text:
        return []

    # Chu·∫©n h√≥a text
    text = _strip_accents(text.lower())
    text = re.sub(r'[^a-z0-9\s]', ' ', text)  # Remove punctuation
    text = re.sub(r'\s+', ' ', text).strip()

    # Lo·∫°i b·ªè stopwords v√† t·ª´ qu√° ng·∫Øn
    words = text.split()
    keywords = []

    for word in words:
        # B·ªè qua stopwords v√† t·ª´ qu√° ng·∫Øn
        if (len(word) <= 2 or
                word in VN_STOPWORDS or
                word.isdigit() or
                any(word in stop for stop in VN_STOPWORDS if len(stop) > 2)):
            continue

        # Th√™m t·ª´ c√≥ √Ω nghƒ©a
        if len(word) >= 3 and word not in keywords:
            keywords.append(word)

    return keywords[:3]  # Gi·ªõi h·∫°n 3 keyword th√¥ng th∆∞·ªùng


def _extract_keywords(message: str) -> List[str]:
    # L·∫•y c√°c phrase ƒë·∫∑c bi·ªát (pet v√† amenity)
    kws = _pick_phrases(message, PET_SYNS)
    msg_wo_geo = _remove_geo_from_text(message)
    kws += [k for k in _pick_phrases(msg_wo_geo, AMENITY_PHRASES) if k not in kws]

    # Th√™m: extract c√°c keyword th√¥ng th∆∞·ªùng t·ª´ message
    regular_kws = _extract_regular_keywords(msg_wo_geo)
    kws.extend([kw for kw in regular_kws if kw not in kws])

    return kws[:5]  # TƒÉng limit l√™n 5


def parse_core(message: str) -> dict:
    province = _find_one_geo(PROVINCES, message)
    district = _find_one_geo(DISTRICTS, message)
    ward = _find_one_geo(WARDS, message)
    pmin, pmax = _parse_price(message)
    kw = _extract_keywords(message)

    return {
        "province": province,
        "district": district,
        "ward": ward,
        "price_min": pmin,
        "price_max": pmax,
        "area_min": None,
        "area_max": None,
        "amenities": [],
        "targets": [],
        "env_types": [],
        "keywords": kw,
        "semantic_query": message
    }


# Helper: canonize a SQL column (unaccent + drop geo prefixes + collapse spaces)
def _sql_canon(col: str) -> str:
    return (
        "regexp_replace("
        "  regexp_replace("
        f"    f_unaccent({col}),"
        "    '(\\y(q\\.?|quan|qu·∫≠n|h\\.?|huyen|huy·ªán|tp|t\\.p\\.?|thanh pho|th√†nh ph·ªë|thi xa|tx|phuong|p\\.?|ph∆∞·ªùng)\\y)',"
        "    ' ', 'gi'"
        "  ),"
        "  '\\s+', ' ', 'g'"
        ")"
    )


def search_core(p: Search) -> List[dict]:
    # Hi·ªÉn th·ªã khoan dung: ch·ªâ lo·∫°i ƒë√∫ng false; true/NULL ƒë·ªÅu qua
    where = ["COALESCE(bz.is_visible, false) = false"]
    args: List = []

    def like_u(expr: str) -> str:
        return f"f_unaccent({expr}) ILIKE f_unaccent(%s)"

    # ===== Province strict (with address fallback) =====
    if p.province:
        canon = _canon_geo(p.province)
        col_canon = _sql_canon("bz.province")
        addr_canon = _sql_canon("bz.address")
        strict = f"(({col_canon} <> '') AND {col_canon} ILIKE %s)"
        fallback = f"(((bz.province IS NULL OR bz.province = '')) AND {addr_canon} ILIKE %s)"
        where.append(f"({strict} OR {fallback})")
        args += [f"%{canon}%", f"%{canon}%"]

    # ===== District strict (with address fallback) =====
    if p.district:
        canon = _canon_geo(p.district)
        col_canon = _sql_canon("bz.district")
        addr_canon = _sql_canon("bz.address")
        strict = f"(({col_canon} <> '') AND {col_canon} ILIKE %s)"
        fallback = f"(((bz.district IS NULL OR bz.district = '')) AND {addr_canon} ILIKE %s)"
        where.append(f"({strict} OR {fallback})")
        args += [f"%{canon}%", f"%{canon}%"]

    # ===== Ward strict (with address fallback) =====
    if p.ward:
        canon = _canon_geo(p.ward)
        col_canon = _sql_canon("bz.ward")
        addr_canon = _sql_canon("bz.address")
        strict = f"(({col_canon} <> '') AND {col_canon} ILIKE %s)"
        fallback = f"(((bz.ward IS NULL OR bz.ward = '')) AND {addr_canon} ILIKE %s)"
        where.append(f"({strict} OR {fallback})")
        args += [f"%{canon}%", f"%{canon}%"]

    # Numeric filters
    if p.price_min is not None:
        where.append("bz.expected_price >= %s");
        args.append(p.price_min)
    if p.price_max is not None:
        where.append("bz.expected_price <= %s");
        args.append(p.price_max)
    if p.area_min is not None:
        where.append("bz.area >= %s");
        args.append(p.area_min)
    if p.area_max is not None:
        where.append("bz.area <= %s");
        args.append(p.area_max)

    # --- Name/Description keywords (AND across keywords) ---
    if p.keywords:
        for kw in p.keywords:
            # S·ª≠ d·ª•ng websearch_to_tsquery cho t√¨m ki·∫øm t·ª± nhi√™n h∆°n
            where.append("""
                    to_tsvector('simple', f_unaccent(
                        coalesce(bz.name,'')||' '||coalesce(bz.description,'')||' '||
                        coalesce(bz.address,'')||' '||coalesce(bz.district,'')||' '||
                        coalesce(bz.ward,'')||' '||coalesce(bz.province,'')
                    )) @@ websearch_to_tsquery('simple', f_unaccent(%s))
                """)
            args.append(kw)

    base = f"""
      SELECT bz.id, bz.name, bz.expected_price, bz.area, bz.province, bz.district, bz.ward,
             bz.address, bz.description, bz.created_at
      FROM boarding_zones bz
      WHERE {' AND '.join(where)}
    """

    with pool.connection() as conn:
        with conn.cursor() as cur:
            if p.semantic_query:
                qvec = embed_vec_literal(p.semantic_query)
                try:
                    cur.execute("SELECT set_config('ivfflat.probes', '8', true)")
                except Exception as e:
                    print("[WARN] cannot set ivfflat.probes:", e)

                sql = f"""
                WITH filtered AS (
                  {base}
                ),
                scored AS (
                  SELECT f.*,
                         ts_rank(
                           to_tsvector('simple', f_unaccent(
                             coalesce(f.name,'')||' '||coalesce(f.description,'')||' '||
                             coalesce(f.address,'')||' '||coalesce(f.district,'')||' '||coalesce(f.ward,'')
                           )),
                           plainto_tsquery('simple', f_unaccent(%s))
                         ) AS fts_score,
                         1 - (bz.embedding <=> %s::vector) AS emb_score
                  FROM filtered f
                  JOIN boarding_zones bz ON bz.id = f.id
                )
                SELECT * FROM scored
                WHERE (COALESCE(fts_score,0) > 0 OR emb_score >= %s)
                ORDER BY (COALESCE(fts_score,0)*0.4 + COALESCE(emb_score,0)*0.6) DESC,
                         created_at DESC
                LIMIT %s OFFSET %s
                """
                cur.execute(sql, args + [p.semantic_query, qvec, SEM_MIN_SIM, p.limit, (p.page - 1) * p.limit])
            else:
                sql = f"{base} ORDER BY created_at DESC LIMIT %s OFFSET %s"
                cur.execute(sql, args + [p.limit, (p.page - 1) * p.limit])

            cols = [c.name for c in cur.description]
            return [dict(zip(cols, r)) for r in cur.fetchall()]


def _build_chat_prompt(msg: str, items: List[dict], context: str) -> str:
    item_count = len(items)

    if item_count == 0:
        return f"""Y√™u c·∫ßu: {msg}

Kh√¥ng t√¨m th·∫•y ph√≤ng n√†o ph√π h·ª£p. H√£y ph√¢n t√≠ch l√Ω do v√† ƒë∆∞a ra 3-4 g·ª£i √Ω c·ª• th·ªÉ ƒë·ªÉ m·ªü r·ªông t√¨m ki·∫øm."""

    elif item_count <= 2:
        return f"""Y√™u c·∫ßu: {msg}

K·∫øt qu·∫£ t√¨m ƒë∆∞·ª£c ({item_count} ph√≤ng):
{context}

Ph√¢n t√≠ch ƒëi·ªÉm ph√π h·ª£p v√† ƒë·ªÅ xu·∫•t c√°ch t√¨m th√™m ph√≤ng t∆∞∆°ng t·ª±."""

    else:
        return f"""Y√™u c·∫ßu: {msg}

K·∫øt qu·∫£ t√¨m ƒë∆∞·ª£c ({item_count} ph√≤ng):
{context}

T√≥m t·∫Øt ph√≤ng n·ªïi b·∫≠t nh·∫•t v√† g·ª£i √Ω ti√™u ch√≠ l·ªçc."""


def _generate_suggestions(items: List[dict], parsed: dict) -> List[str]:
    """T·∫°o g·ª£i √Ω t·ª± ƒë·ªông d·ª±a tr√™n k·∫øt qu·∫£"""
    suggestions = []

    if not items:
        if parsed.get('price_min'):
            suggestions.append("Th·ª≠ m·ªü r·ªông kho·∫£ng gi√° ¬±20%")
        if parsed.get('district'):
            suggestions.append("T√¨m c√°c qu·∫≠n l√¢n c·∫≠n")
        suggestions.append("Gi·∫£m b·ªõt y√™u c·∫ßu v·ªÅ ti·ªán nghi")

    elif len(items) < 3:
        suggestions.append("T√¨m th√™m ph√≤ng v·ªõi ti√™u ch√≠ t∆∞∆°ng t·ª±")

    return suggestions[:3]


# ====== SYSTEM PROMPT ======
system = """B·∫°n l√† chatbot t∆∞ v·∫•n ph√≤ng tr·ªç th√¥ng minh. H√£y:

1. PH√ÇN T√çCH NG·ªÆ C·∫¢NH:
- N·∫øu c√≥ k·∫øt qu·∫£: gi·∫£i th√≠ch ƒëi·ªÉm ph√π h·ª£p, so s√°nh ∆∞u ƒëi·ªÉm
- N·∫øu kh√¥ng c√≥ k·∫øt qu·∫£: g·ª£i √Ω m·ªü r·ªông ti√™u ch√≠
- N·∫øu √≠t k·∫øt qu·∫£: ƒë·ªÅ xu·∫•t ƒëi·ªÅu ch·ªânh t√¨m ki·∫øm

2. PHONG C√ÅCH:
- Th√¢n thi·ªán, nhi·ªát t√¨nh nh∆∞ng chuy√™n nghi·ªáp
- D√πng ti·∫øng Vi·ªát t·ª± nhi√™n, g·∫ßn g≈©i
- K√®m emoji ph√π h·ª£p üè†üí∞üîç

3. C·∫§U TR√öC:
- Ch√†o h·ªèi ng·∫Øn g·ªçn
- Ph√¢n t√≠ch k·∫øt qu·∫£ (n·∫øu c√≥)
- G·ª£i √Ω c·ª• th·ªÉ ƒë·ªÉ c·∫£i thi·ªán k·∫øt qu·∫£
- K·∫øt th√∫c t√≠ch c·ª±c"""


# ====== ENDPOINTS ======
@app.post("/parse")
def parse(body: dict):
    t0 = time.perf_counter()
    out = parse_core((body.get("message") or "").strip())
    timing("parse", t0)
    return out


@app.post("/search")
def search(p: Search):
    t0 = time.perf_counter()
    pmin, pmax = widen_price(p.price_min, p.price_max)
    p = p.copy(update={"price_min": pmin, "price_max": pmax})
    items = search_core(p)
    timing("search", t0)
    return {"items": items}


@app.post("/chat")
def chat(body: dict):
    t_all = time.perf_counter()
    msg = (body.get("message") or "").strip()
    chat_history = body.get("history", [])

    try:
        t0 = time.perf_counter()
        parsed = parse_core(msg)
        t1 = timing("chat.parse", t0)

        pmin, pmax = widen_price(parsed.get("price_min"), parsed.get("price_max"))
        params = Search(
            province=parsed.get("province"),
            district=parsed.get("district"),
            ward=parsed.get("ward"),
            price_min=pmin, price_max=pmax,
            area_min=parsed.get("area_min"),
            area_max=parsed.get("area_max"),
            amenities=parsed.get("amenities") or [],
            targets=parsed.get("targets") or [],
            env_types=parsed.get("env_types") or [],
            limit=8, page=1,
            keywords=parsed.get("keywords") or [],
            semantic_query=parsed.get("semantic_query") or msg,
        )
        items = search_core(params)
        t2 = timing("chat.search", t1)

        # Build context
        lines = []
        for i, r in enumerate(items[:6]):
            price = f"{r.get('expected_price', 0):,.0f}ƒë".replace(",", ".")
            district_ward = f"{r.get('district', '')} {r.get('ward', '')}".strip()
            desc = (r.get('description') or '')[:100]
            lines.append(f"{i + 1}. {r['name']} | {price} | {district_ward}\n   {desc}...")

        context = "\n\n".join(lines) if lines else "Kh√¥ng t√¨m th·∫•y ph√≤ng n√†o ph√π h·ª£p."
        prompt = _build_chat_prompt(msg, items, context)

        if chat_history:
            history_context = "\n".join([f"User: {h.get('user', '')}\nBot: {h.get('bot', '')}"
                                         for h in chat_history[-3:]])
            prompt = f"""L·ªäCH S·ª¨ CHAT:
{history_context}

HI·ªÜN T·∫†I:
{prompt}"""

        answer = ollama_generate(CHAT_MODEL, system, prompt, as_json=False, timeout=90)
        timing("chat.answer", t2)
        timing("chat.total", t_all)

        return {
            "answer": answer,
            "items": items[:5],
            "suggestions": _generate_suggestions(items, parsed)
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat/v2")
def chat_v2(chat_data: ChatHistory):
    """Phi√™n b·∫£n chat h·ªó tr·ª£ history"""
    return chat({"message": chat_data.message, "history": chat_data.history})